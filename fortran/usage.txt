In this file, it is explained how to call ASA-BCP in Fortran to solve

                               min f(x)
                          s.t. l <= x <= u

with given vectors l, u and where f(x) is a twice continuously
differentiable function.
-------------------------------------------------------------------------


ASA-BCP is implemented in the subroutine 'asa_bcp', defined in the file
'asa_bcp.f90'.

The subroutine ASA_BCP(N,X,F,L,U,OPTS,FLAG) tries to minimize an objective
function of N variables subject to bound constraints L <= X <= U. There is
no i-th lower bound if L(i) <= -1.d20 and there is no i-th upper bound if
U(i) >= 1.d20. Algorithm parameters are specified in OPTS, which is an
object of derived data type 'asa_bcp_options' (see below). X is the
starting point as input and the final solution found by the algorithm as
output. F is the objective value at the final solution. FLAG describes the
exit condition:
 -1 if the problem is infeasible,
  0 if the sup-norm of x - p[x-g(x)] <= 'eps_opt'
    (default value of 'eps_opt' = 1.d-5),
  1 if the directional derivative of the objective function along the
    search direction (in absolute value) <= 'min_gd'
    (default value of 'min_gd' = 1.d-15),
  2 if the norm of the projected search direction <= 'min_norm_proj_d'
    (default value of 'min_norm_proj_d' = 1.d-9),
  3 if the stepsize <= 'min_stepsize'
    (default value of 'min_stepsize' = 1.d-20),
  4 if the number of iterations >= 'max_it'
    (default value of 'max_it' = 1000000),
  5 if the number of function evaluations >= 'max_n_f'
    (default value of 'max_n_f' = 1000000),
  6 if the number of gradient evaluations >= 'max_n_g'
    (default value of 'max_n_g' = 1000000),
  7 if the number of Hessian-vector products >= 'max_n_hd'
    (default value of 'max_n_hd' = 1000000),
  8 if the objective value <= 'min_f'
    (default value of 'min_f' = -1.d90),
  9 in case of error when computing the objective function,
 10 in case of error when computing the gradient of the objective
    function,
 11 in case of error when computing the Hessian-vector product.

To set the problem, you have to define
- a subroutine 'funct' that computes the value of the objective function
  at a given point,
- a subroutine 'grad' that computes the gradient of the objective function
  at a given point,
- a subroutine 'hd_prod' that computes the product of a given vector with
  the Hessian matrix of the objective function at a given point.

See the file 'problem.f90' to know input/output arguments of 'funct',
'grad' and 'hd_prod'.

The subroutine 'hd_prod' can be ignored (in the sense that it can return
any dummy value) if Hessian-vector products are approximated (see below
the parameter 'hd_exact').

Other output values from ASA_BCP can be obtained by the following
subroutines:
- 'get_sup_norm_proj_g', which returns the sup-norm of X - P[X-G(X)],
   where X is the solution found by the algorithm, P[.] is the projection
   operator onto [l,u] and G(.) is the gradient of the objective function;
- 'get_it', which returns the number of iterations;
- 'get_inner_it', which returns the number of inner conjugate gradient
   iterations;
- 'get_n_f', which returns the number of function evaluations;
- 'get_n_g', which returns the number of gradient evaluations;
- 'get_n_hd', which returns the number of Hessian-vector products.

To change the values of the above parameters 'eps_opt', 'min_gd',
'min_proj_d', 'min_stepsize', 'max_it', 'max_n_f', 'max_n_g', 'max_n_hd'
and 'min_f', create an object of derived data type asa_bcp_options (see
its declaration in the module 'asa_bcp_opts') and assign new values to
(some of) its members.
Other parameters that can be changed in the same way are the following:
- 'ls_memory': history length for the computation of the reference value in
               the line search: set 'ls_memory' > 1 for non-monotone line
               search, or 'ls_memory' = 1 for monotone line search
               (default value of 'm' = 100);
- 'z': according to the non-monotone strategy, the objective function must
       be evaluated upon at least once every 'z' iterations
       (default value of 'z' = 20);
- 'hd_exact': .true. to compute exact Hessian-vector products, .false. to
              approximate them
              (default value of 'hd_exact' = .true.);
- 'verbosity': 0 for no prints, 1 for synthetic prints, 2 for detailed
               prints
               (default value of 'verbosity' = 1).

If 'verbosity' is positive, then the iteration details will be also
reported in the file 'iteration_history.txt'.

Finally, if 'hd_exact' is .false., the Hessian-vector product H(x)*d is
approximated with [g(x+eps_approx*d)-g(x)]/eps_approx, where g(.) is the
gradient of the objective function and the default value of 'eps_approx'
is 1.d-6.
